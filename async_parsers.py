import asyncio
import aiohttp
from bs4 import BeautifulSoup
from urllib.parse import quote
from config import ITEMS_PER_PAGE, logger
from utils import (
    generate_item_id, make_full_url, get_next_user_agent,
    get_next_proxy_async, mark_proxy_bad_str
)
from playwright_manager import fetch_html_playwright  # –∑–∞–º–µ–Ω–∏–ª–∏ playwright_fallback

# ==================== –ë—ã—Å—Ç—Ä—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å –ø—Ä–æ–∫—Å–∏ ====================
async def fetch_html(session, url, semaphore, timeout=15, retries=3):
    # ... (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π, –∫–∞–∫ –≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–º async_parsers)
    async with semaphore:
        headers = {
            'User-Agent': get_next_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        for attempt in range(retries):
            proxy = await get_next_proxy_async()
            try:
                async with session.get(url, headers=headers, proxy=proxy, timeout=timeout, ssl=False) as response:
                    if response.status == 200:
                        return await response.text()
                    elif response.status in [403, 404]:
                        logger.warning(f"üö´ {response.status} –¥–ª—è {url[:100]}...")
                        return None
                    else:
                        logger.warning(f"üåê HTTP {response.status} –¥–ª—è {url[:100]}...")
            except asyncio.TimeoutError:
                logger.warning(f"‚è∞ –¢–∞–π–º–∞—É—Ç (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}) –¥–ª—è {url[:100]}...")
            except aiohttp.ClientProxyConnectionError as e:
                logger.warning(f"üîå –û—à–∏–±–∫–∞ –ø—Ä–æ–∫—Å–∏ {proxy} (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}): {e}")
                if proxy:
                    loop = asyncio.get_running_loop()
                    await loop.run_in_executor(None, mark_proxy_bad_str, proxy)
            except aiohttp.ClientConnectorError as e:
                logger.warning(f"üîå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}): {e}")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url[:100]}: {e}")
            if attempt < retries - 1:
                await asyncio.sleep(2 ** attempt)
        return None

# ==================== –ì–∏–±—Ä–∏–¥–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞: —Å–Ω–∞—á–∞–ª–∞ –±—ã—Å—Ç—Ä—ã–π fetch, –ø—Ä–∏ –Ω–µ—É–¥–∞—á–µ ‚Äî Playwright ====================
async def fetch_with_fallback(session, url, semaphore, expected_selector=None, use_playwright=True):
    html = await fetch_html(session, url, semaphore)
    if html:
        if expected_selector:
            soup = BeautifulSoup(html, 'lxml')
            if soup.select_one(expected_selector):
                return html
            else:
                logger.warning(f"‚ö†Ô∏è –ë—ã—Å—Ç—Ä—ã–π –∑–∞–ø—Ä–æ—Å —É—Å–ø–µ—à–µ–Ω, –Ω–æ —Å–µ–ª–µ–∫—Ç–æ—Ä '{expected_selector}' –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–±—É—é Playwright.")
        else:
            return html

    if use_playwright:
        logger.info(f"üîÑ Fallback to Playwright for {url[:100]}...")
        html = await fetch_html_playwright(url, expected_selector=expected_selector)
        return html
    return None

# ==================== –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫–∞—Ä—Ç–æ—á–∫–∏ ====================
def extract_item_from_card(card, source, base_url, title_sel, price_sel, link_sel='a', img_sel='img'):
    # ... (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
    try:
        title_elem = card.select_one(title_sel)
        price_elem = card.select_one(price_sel)
        link_elem = card.select_one(link_sel)
        img_elem = card.select_one(img_sel) if img_sel else None

        if not title_elem or not link_elem:
            return None

        title = title_elem.text.strip()
        price = price_elem.text.strip() if price_elem else '–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞'

        img_url = None
        if img_elem:
            img_url = img_elem.get('src')
            if img_url and img_url.startswith('//'):
                img_url = 'https:' + img_url

        href = link_elem.get('href')
        full_url = make_full_url(base_url, href)

        return {
            'title': title[:100],
            'price': price[:50],
            'url': full_url,
            'img_url': img_url,
            'source': source
        }
    except Exception as e:
        logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ä—Ç–æ—á–∫–∏ {source}: {e}")
        return None

# ==================== –ü–∞—Ä—Å–µ—Ä—ã (–∫–∞–∂–¥—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç fetch_with_fallback) ====================
async def parse_mercari_async(session, keyword, semaphore):
    items = []
    url = f"https://jp.mercari.com/search?keyword={quote(keyword)}&order=desc&sort=created_time"
    html = await fetch_with_fallback(session, url, semaphore, expected_selector='[data-testid="item-cell"]')
    if not html:
        return items
    try:
        soup = BeautifulSoup(html, 'lxml')
        cards = soup.select('[data-testid="item-cell"]')[:ITEMS_PER_PAGE]
        for card in cards:
            item_data = extract_item_from_card(
                card,
                source='Mercari JP',
                base_url='https://jp.mercari.com',
                title_sel='[data-testid="thumbnail-title"]',
                price_sel='[data-testid="price"]',
                link_sel='a',
                img_sel='img'
            )
            if item_data:
                item_data['id'] = generate_item_id(item_data)
                items.append(item_data)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Mercari –¥–ª—è {keyword}: {e}")
    return items

# –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –ø–ª–æ—â–∞–¥–æ–∫ (Rakuma, Yahoo Flea, Yahoo Auction, Yahoo Shopping, Rakuten Mall, eBay, 2nd Street)
# (–∑–¥–µ—Å—å –Ω—É–∂–Ω–æ —Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –≤–µ—Ä—Å–∏–∏ async_parsers, –∑–∞–º–µ–Ω–∏–≤ fetch_html –Ω–∞ fetch_with_fallback)

# ==================== –°–ª–æ–≤–∞—Ä—å –ø–∞—Ä—Å–µ—Ä–æ–≤ ====================
ASYNC_PARSERS = {
    'Mercari JP': parse_mercari_async,
    'Rakuten Rakuma': parse_rakuma_async,
    'Yahoo Flea': parse_yahoo_flea_async,
    'Yahoo Auction': parse_yahoo_auction_async,
    'Yahoo Shopping': parse_yahoo_shopping_async,
    'Rakuten Mall': parse_rakuten_mall_async,
    'eBay': parse_ebay_async,
    '2nd Street JP': parse_2ndstreet_async,
}

# ==================== –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ ====================
async def search_all_async(keywords, platforms, max_concurrent=20):
    semaphore = asyncio.Semaphore(max_concurrent)
    connector = aiohttp.TCPConnector(limit=100, limit_per_host=10, ttl_dns_cache=300, ssl=False)
    timeout = aiohttp.ClientTimeout(total=30)
    
    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
        tasks = []
        for platform in platforms:
            if platform in ASYNC_PARSERS:
                parser = ASYNC_PARSERS[platform]
                for keyword in keywords:
                    tasks.append(parser(session, keyword, semaphore))
        
        logger.info(f"üöÄ –ó–∞–ø—É—â–µ–Ω–æ {len(tasks)} –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á")
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        all_items = []
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"–û—à–∏–±–∫–∞ –≤ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –∑–∞–¥–∞—á–µ: {result}")
            elif isinstance(result, list):
                all_items.extend(result)
        
        logger.info(f"‚úÖ –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω, –Ω–∞–π–¥–µ–Ω–æ {len(all_items)} —Ç–æ–≤–∞—Ä–æ–≤")
        return all_items

# ==================== –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∏–∑ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∞ ====================
def run_async_search(keywords, platforms, max_concurrent=20):
    # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π —Ü–∏–∫–ª –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π? –õ—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±—â–∏–π.
    # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º run_coro –∏–∑ async_loop, —á—Ç–æ–±—ã –≤—ã–ø–æ–ª–Ω–∏—Ç—å –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ü–∏–∫–ª–µ.
    from async_loop import run_coro
    future = run_coro(search_all_async(keywords, platforms, max_concurrent))
    return future.result()  # –∂–¥—ë–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç