import asyncio
import aiohttp
from bs4 import BeautifulSoup
from urllib.parse import quote
from config import ITEMS_PER_PAGE, logger
from utils import (
    generate_item_id, make_full_url, get_next_user_agent,
    get_next_proxy_async, mark_proxy_bad_str
)
from playwright_manager import fetch_html_playwright

try:
    import brotli
except ImportError:
    logger.warning("Brotli not installed, some sites may fail. Run: pip install brotli")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Å–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (HTTP + Playwright)
GLOBAL_SEMAPHORE = asyncio.Semaphore(10)

# –û—Ç–¥–µ–ª—å–Ω—ã–π —Å–µ–º–∞—Ñ–æ—Ä –¥–ª—è Playwright (—á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å –±—Ä–∞—É–∑–µ—Ä)
PLAYWRIGHT_SEMAPHORE = asyncio.Semaphore(2)

async def fetch_html(session, url):
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç HTTP-–∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ aiohttp —Å –ø—Ä–æ–∫—Å–∏ –∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Å–µ–º–∞—Ñ–æ—Ä.
    """
    async with GLOBAL_SEMAPHORE:
        headers = {
            'User-Agent': get_next_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        for attempt in range(3):
            proxy = await get_next_proxy_async()
            try:
                async with session.get(url, headers=headers, proxy=proxy, timeout=15, ssl=False) as response:
                    if response.status == 200:
                        return await response.text()
                    elif response.status in [403, 404]:
                        logger.warning(f"üö´ {response.status} –¥–ª—è {url[:100]}...")
                        return None
                    else:
                        logger.warning(f"üåê HTTP {response.status} –¥–ª—è {url[:100]}...")
            except asyncio.TimeoutError:
                logger.warning(f"‚è∞ –¢–∞–π–º–∞—É—Ç (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}) –¥–ª—è {url[:100]}...")
            except aiohttp.ClientProxyConnectionError as e:
                logger.warning(f"üîå –û—à–∏–±–∫–∞ –ø—Ä–æ–∫—Å–∏ {proxy} (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}): {e}")
                if proxy:
                    loop = asyncio.get_running_loop()
                    await loop.run_in_executor(None, mark_proxy_bad_str, proxy)
            except aiohttp.ClientConnectorError as e:
                logger.warning(f"üîå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}): {e}")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url[:100]}: {e}")
            await asyncio.sleep(2 ** attempt)
        return None

async def fetch_with_fallback(session, url, expected_selector=None, use_playwright=True):
    """
    –ü—ã—Ç–∞–µ—Ç—Å—è —Å–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É —á–µ—Ä–µ–∑ HTTP. –ï—Å–ª–∏ –Ω–µ —É–¥–∞—ë—Ç—Å—è –∏–ª–∏ –Ω–µ—Ç —Å–µ–ª–µ–∫—Ç–æ—Ä–∞,
    –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Playwright (—Å –æ—Ç–¥–µ–ª—å–Ω—ã–º —Å–µ–º–∞—Ñ–æ—Ä–æ–º).
    """
    html = await fetch_html(session, url)
    if html:
        if expected_selector:
            soup = BeautifulSoup(html, 'lxml')
            if soup.select_one(expected_selector):
                return html
            else:
                logger.warning(f"‚ö†Ô∏è –ë—ã—Å—Ç—Ä—ã–π –∑–∞–ø—Ä–æ—Å —É—Å–ø–µ—à–µ–Ω, –Ω–æ —Å–µ–ª–µ–∫—Ç–æ—Ä '{expected_selector}' –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–±—É—é Playwright.")
        else:
            return html

    if use_playwright:
        async with PLAYWRIGHT_SEMAPHORE:
            async with GLOBAL_SEMAPHORE:
                logger.info(f"üîÑ Fallback to Playwright for {url[:100]}...")
                html = await fetch_html_playwright(url, expected_selector=expected_selector)
                return html
    return None

def extract_item_from_card(card, source, base_url, title_sel, price_sel, link_sel='a', img_sel='img'):
    try:
        title_elem = card.select_one(title_sel)
        price_elem = card.select_one(price_sel)
        link_elem = card.select_one(link_sel)
        img_elem = card.select_one(img_sel) if img_sel else None

        if not title_elem or not link_elem:
            return None

        title = title_elem.text.strip()
        price = price_elem.text.strip() if price_elem else '–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞'

        img_url = None
        if img_elem:
            img_url = img_elem.get('src')
            if img_url and img_url.startswith('//'):
                img_url = 'https:' + img_url

        href = link_elem.get('href')
        full_url = make_full_url(base_url, href)

        return {
            'title': title[:100],
            'price': price[:50],
            'url': full_url,
            'img_url': img_url,
            'source': source
        }
    except Exception as e:
        logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ä—Ç–æ—á–∫–∏ {source}: {e}")
        return None

# ==================== –ü–∞—Ä—Å–µ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–π –ø–ª–æ—â–∞–¥–∫–∏ ====================

async def parse_mercari_async(session, keyword):
    items = []
    url = f"https://jp.mercari.com/search?keyword={quote(keyword)}&order=desc&sort=created_time"
    html = await fetch_with_fallback(session, url, expected_selector='[data-testid="item-cell"]')
    if not html:
        return items
    try:
        soup = BeautifulSoup(html, 'lxml')
        cards = soup.select('[data-testid="item-cell"]')[:ITEMS_PER_PAGE]
        for card in cards:
            item_data = extract_item_from_card(
                card,
                source='Mercari JP',
                base_url='https://jp.mercari.com',
                title_sel='[data-testid="thumbnail-title"]',
                price_sel='[data-testid="price"]',
                link_sel='a',
                img_sel='img'
            )
            if item_data:
                item_data['id'] = generate_item_id(item_data)
                items.append(item_data)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Mercari –¥–ª—è {keyword}: {e}")
    return items

async def parse_rakuma_async(session, keyword):
    items = []
    url = f"https://fril.jp/s?query={quote(keyword)}&order=desc&sort=created_at"
    html = await fetch_with_fallback(session, url, expected_selector='.item')
    if not html:
        return items
    try:
        soup = BeautifulSoup(html, 'lxml')
        cards = soup.select('.item')[:ITEMS_PER_PAGE]
        for card in cards:
            item_data = extract_item_from_card(
                card,
                source='Rakuten Rakuma',
                base_url='https://fril.jp',
                title_sel='.item-box__title a',
                price_sel='.item-box__price',
                link_sel='a',
                img_sel='img'
            )
            if item_data:
                item_data['id'] = generate_item_id(item_data)
                items.append(item_data)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Rakuma –¥–ª—è {keyword}: {e}")
    return items

async def parse_yahoo_flea_async(session, keyword):
    items = []
    url = f"https://paypayfleamarket.yahoo.co.jp/search/{quote(keyword)}?order=desc&sort=create_time"
    html = await fetch_with_fallback(session, url, expected_selector='.Product')
    if not html:
        return items
    try:
        soup = BeautifulSoup(html, 'lxml')
        cards = soup.select('.Product')[:ITEMS_PER_PAGE]
        for card in cards:
            item_data = extract_item_from_card(
                card,
                source='Yahoo Flea',
                base_url='https://paypayfleamarket.yahoo.co.jp',
                title_sel='.Product__titleLink',
                price_sel='.Product__price',
                link_sel='a',
                img_sel='img'
            )
            if item_data:
                item_data['id'] = generate_item_id(item_data)
                items.append(item_data)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Yahoo Flea –¥–ª—è {keyword}: {e}")
    return items

async def parse_yahoo_auction_async(session, keyword):
    items = []
    url = f"https://auctions.yahoo.co.jp/search/search?p={quote(keyword)}&aq=-1&type=all&auccat=&tab_ex=commerce&order=desc"
    html = await fetch_with_fallback(session, url, expected_selector='.Product')
    if not html:
        return items
    try:
        soup = BeautifulSoup(html, 'lxml')
        cards = soup.select('.Product')[:ITEMS_PER_PAGE]
        for card in cards:
            item_data = extract_item_from_card(
                card,
                source='Yahoo Auction',
                base_url='https://auctions.yahoo.co.jp',
                title_sel='.Product__titleLink',
                price_sel='.Product__price',
                link_sel='a',
                img_sel='img'
            )
            if item_data:
                item_data['id'] = generate_item_id(item_data)
                items.append(item_data)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Yahoo Auction –¥–ª—è {keyword}: {e}")
    return items

async def parse_yahoo_shopping_async(session, keyword):
    items = []
    url = f"https://shopping.yahoo.co.jp/search?p={quote(keyword)}&used=1&order=desc&sort=create_time"
    html = await fetch_with_fallback(session, url, expected_selector='.Loop__item')
    if not html:
        return items
    try:
        soup = BeautifulSoup(html, 'lxml')
        cards = soup.select('.Loop__item')[:ITEMS_PER_PAGE]
        for card in cards:
            item_data = extract_item_from_card(
                card,
                source='Yahoo Shopping',
                base_url='https://shopping.yahoo.co.jp',
                title_sel='.Loop__itemTitle a',
                price_sel='.Loop__itemPrice',
                link_sel='a',
                img_sel='img'
            )
            if item_data:
                item_data['id'] = generate_item_id(item_data)
                items.append(item_data)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Yahoo Shopping –¥–ª—è {keyword}: {e}")
    return items

async def parse_rakuten_mall_async(session, keyword):
    items = []
    encoded = quote(keyword, safe='')
    url = f"https://search.rakuten.co.jp/search/mall/{encoded}/?used=1"
    html = await fetch_with_fallback(session, url, expected_selector='.searchresultitem')
    if not html:
        alt_url = f"https://search.rakuten.co.jp/search/mall/?v=2&p={encoded}&used=1"
        html = await fetch_with_fallback(session, alt_url, expected_selector='.searchresultitem')
        if not html:
            return items
    try:
        soup = BeautifulSoup(html, 'lxml')
        cards = soup.select('.searchresultitem')[:ITEMS_PER_PAGE]
        for card in cards:
            item_data = extract_item_from_card(
                card,
                source='Rakuten Mall',
                base_url='https://search.rakuten.co.jp',
                title_sel='.title a',
                price_sel='.important',
                link_sel='a',
                img_sel='img'
            )
            if item_data:
                item_data['id'] = generate_item_id(item_data)
                items.append(item_data)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Rakuten Mall –¥–ª—è {keyword}: {e}")
    return items

async def parse_ebay_async(session, keyword):
    items = []
    url = f"https://www.ebay.com/sch/i.html?_nkw={quote(keyword)}&_sacat=11450&LH_ItemCondition=4&_sop=10"
    html = await fetch_with_fallback(session, url, expected_selector='li.s-item')
    if not html:
        return items
    try:
        soup = BeautifulSoup(html, 'lxml')
        cards = soup.select('li.s-item')[:ITEMS_PER_PAGE]
        for card in cards:
            title_elem = card.select_one('.s-item__title')
            if not title_elem or 'Shop on' in title_elem.text:
                continue
            item_data = extract_item_from_card(
                card,
                source='eBay',
                base_url='https://www.ebay.com',
                title_sel='.s-item__title',
                price_sel='.s-item__price',
                link_sel='a.s-item__link',
                img_sel='.s-item__image-img'
            )
            if item_data:
                item_data['id'] = generate_item_id(item_data)
                items.append(item_data)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ eBay –¥–ª—è {keyword}: {e}")
    return items

async def parse_2ndstreet_async(session, keyword):
    items = []
    url = f"https://www.2ndstreet.jp/search?keyword={quote(keyword)}"
    html = await fetch_with_fallback(session, url, expected_selector='.itemList .item')
    if not html:
        return items
    try:
        soup = BeautifulSoup(html, 'lxml')
        cards = soup.select('.itemList .item')[:ITEMS_PER_PAGE]
        for card in cards:
            item_data = extract_item_from_card(
                card,
                source='2nd Street JP',
                base_url='https://www.2ndstreet.jp',
                title_sel='.itemName',
                price_sel='.price',
                link_sel='a',
                img_sel='img'
            )
            if item_data:
                item_data['id'] = generate_item_id(item_data)
                items.append(item_data)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ 2nd Street –¥–ª—è {keyword}: {e}")
    return items

# ==================== –°–ª–æ–≤–∞—Ä—å –ø–∞—Ä—Å–µ—Ä–æ–≤ ====================
ASYNC_PARSERS = {
    'Mercari JP': parse_mercari_async,
    'Rakuten Rakuma': parse_rakuma_async,
    'Yahoo Flea': parse_yahoo_flea_async,
    'Yahoo Auction': parse_yahoo_auction_async,
    'Yahoo Shopping': parse_yahoo_shopping_async,
    'Rakuten Mall': parse_rakuten_mall_async,
    'eBay': parse_ebay_async,
    '2nd Street JP': parse_2ndstreet_async,
}

# ==================== –§—É–Ω–∫—Ü–∏—è —Å –æ—á–µ—Ä–µ–¥—å—é –∏ –≤–æ—Ä–∫–µ—Ä–∞–º–∏ ====================
async def worker(queue, session, results):
    while True:
        task = await queue.get()
        if task is None:
            break
        platform, keyword = task
        parser = ASYNC_PARSERS.get(platform)
        if parser:
            try:
                items = await parser(session, keyword)
                results.extend(items)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {platform}/{keyword}: {e}")
        queue.task_done()

async def search_all_async(keywords, platforms, max_workers=5):
    queue = asyncio.Queue()
    results = []

    for platform in platforms:
        if platform not in ASYNC_PARSERS:
            continue
        for keyword in keywords:
            await queue.put((platform, keyword))

    connector = aiohttp.TCPConnector(limit=100, limit_per_host=5, ttl_dns_cache=300, ssl=False)
    timeout = aiohttp.ClientTimeout(total=30)
    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
        workers = [asyncio.create_task(worker(queue, session, results)) for _ in range(max_workers)]
        await queue.join()
        for _ in workers:
            await queue.put(None)
        await asyncio.gather(*workers)

    logger.info(f"‚úÖ –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω, –Ω–∞–π–¥–µ–Ω–æ {len(results)} —Ç–æ–≤–∞—Ä–æ–≤")
    return results

# ==================== –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∏–∑ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∞ ====================
def run_async_search(keywords, platforms, max_workers=5):
    from async_loop import run_coro
    future = run_coro(search_all_async(keywords, platforms, max_workers))
    return future.result()