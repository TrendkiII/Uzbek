import asyncio
import aiohttp
from bs4 import BeautifulSoup
from urllib.parse import quote
from config import ITEMS_PER_PAGE, logger
from utils import (
    generate_item_id, make_full_url, get_next_user_agent,
    get_next_proxy_async, mark_proxy_bad_str
)
from playwright_manager import fetch_html_playwright

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Brotli –¥–ª—è aiohttp (–Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –≤ requirements.txt)
try:
    import brotli
except ImportError:
    logger.warning("Brotli not installed, some sites may fail. Run: pip install brotli")

# –°–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö Playwright –∑–∞–ø—Ä–æ—Å–æ–≤ (—á—Ç–æ–±—ã –Ω–µ —É–±–∏–≤–∞—Ç—å –ø–∞–º—è—Ç—å)
PLAYWRIGHT_SEMAPHORE = asyncio.Semaphore(1)  # ‚ö° —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ

async def fetch_html(session, url, semaphore, timeout=15, retries=3):
    async with semaphore:
        headers = {
            'User-Agent': get_next_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',  # —É–∫–∞–∑—ã–≤–∞–µ–º, —á—Ç–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º brotli
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        for attempt in range(retries):
            proxy = await get_next_proxy_async()
            try:
                async with session.get(url, headers=headers, proxy=proxy, timeout=timeout, ssl=False) as response:
                    if response.status == 200:
                        return await response.text()
                    elif response.status in [403, 404]:
                        logger.warning(f"üö´ {response.status} –¥–ª—è {url[:100]}...")
                        return None
                    else:
                        logger.warning(f"üåê HTTP {response.status} –¥–ª—è {url[:100]}...")
            except asyncio.TimeoutError:
                logger.warning(f"‚è∞ –¢–∞–π–º–∞—É—Ç (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}) –¥–ª—è {url[:100]}...")
            except aiohttp.ClientProxyConnectionError as e:
                logger.warning(f"üîå –û—à–∏–±–∫–∞ –ø—Ä–æ–∫—Å–∏ {proxy} (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}): {e}")
                if proxy:
                    loop = asyncio.get_running_loop()
                    await loop.run_in_executor(None, mark_proxy_bad_str, proxy)
            except aiohttp.ClientConnectorError as e:
                logger.warning(f"üîå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}): {e}")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url[:100]}: {e}")
            if attempt < retries - 1:
                await asyncio.sleep(2 ** attempt)
        return None

# ==================== –ì–∏–±—Ä–∏–¥–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º Playwright ====================
async def fetch_with_fallback(session, url, semaphore, expected_selector=None, use_playwright=True):
    html = await fetch_html(session, url, semaphore)
    if html:
        if expected_selector:
            soup = BeautifulSoup(html, 'lxml')
            if soup.select_one(expected_selector):
                return html
            else:
                logger.warning(f"‚ö†Ô∏è –ë—ã—Å—Ç—Ä—ã–π –∑–∞–ø—Ä–æ—Å —É—Å–ø–µ—à–µ–Ω, –Ω–æ —Å–µ–ª–µ–∫—Ç–æ—Ä '{expected_selector}' –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–±—É—é Playwright.")
        else:
            return html

    if use_playwright:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π —Å–µ–º–∞—Ñ–æ—Ä –¥–ª—è Playwright
        async with PLAYWRIGHT_SEMAPHORE:
            logger.info(f"üîÑ Fallback to Playwright for {url[:100]}...")
            html = await fetch_html_playwright(url, expected_selector=expected_selector)
            return html
    return None

# ==================== –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫–∞—Ä—Ç–æ—á–∫–∏ ====================
def extract_item_from_card(card, source, base_url, title_sel, price_sel, link_sel='a', img_sel='img'):
    try:
        title_elem = card.select_one(title_sel)
        price_elem = card.select_one(price_sel)
        link_elem = card.select_one(link_sel)
        img_elem = card.select_one(img_sel) if img_sel else None

        if not title_elem or not link_elem:
            return None

        title = title_elem.text.strip()
        price = price_elem.text.strip() if price_elem else '–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞'

        img_url = None
        if img_elem:
            img_url = img_elem.get('src')
            if img_url and img_url.startswith('//'):
                img_url = 'https:' + img_url

        href = link_elem.get('href')
        full_url = make_full_url(base_url, href)

        return {
            'title': title[:100],
            'price': price[:50],
            'url': full_url,
            'img_url': img_url,
            'source': source
        }
    except Exception as e:
        logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ä—Ç–æ—á–∫–∏ {source}: {e}")
        return None

# ==================== –ü–∞—Ä—Å–µ—Ä—ã (—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º fetch_with_fallback) ====================

async def parse_mercari_async(session, keyword, semaphore):
    items = []
    url = f"https://jp.mercari.com/search?keyword={quote(keyword)}&order=desc&sort=created_time"
    html = await fetch_with_fallback(session, url, semaphore, expected_selector='[data-testid="item-cell"]')
    if not html:
        return items
    try:
        soup = BeautifulSoup(html, 'lxml')
        cards = soup.select('[data-testid="item-cell"]')[:ITEMS_PER_PAGE]
        for card in cards:
            item_data = extract_item_from_card(
                card,
                source='Mercari JP',
                base_url='https://jp.mercari.com',
                title_sel='[data-testid="thumbnail-title"]',
                price_sel='[data-testid="price"]',
                link_sel='a',
                img_sel='img'
            )
            if item_data:
                item_data['id'] = generate_item_id(item_data)
                items.append(item_data)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Mercari –¥–ª—è {keyword}: {e}")
    return items

async def parse_rakuma_async(session, keyword, semaphore):
    items = []
    url = f"https://fril.jp/s?query={quote(keyword)}&order=desc&sort=created_at"
    html = await fetch_with_fallback(session, url, semaphore, expected_selector='.item')
    if not html:
        return items
    try:
        soup = BeautifulSoup(html, 'lxml')
        cards = soup.select('.item')[:ITEMS_PER_PAGE]
        for card in cards:
            item_data = extract_item_from_card(
                card,
                source='Rakuten Rakuma',
                base_url='https://fril.jp',
                title_sel='.item-box__title a',
                price_sel='.item-box__price',
                link_sel='a',
                img_sel='img'
            )
            if item_data:
                item_data['id'] = generate_item_id(item_data)
                items.append(item_data)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Rakuma –¥–ª—è {keyword}: {e}")
    return items

async def parse_yahoo_flea_async(session, keyword, semaphore):
    items = []
    url = f"https://paypayfleamarket.yahoo.co.jp/search/{quote(keyword)}?order=desc&sort=create_time"
    html = await fetch_with_fallback(session, url, semaphore, expected_selector='.Product')
    if not html:
        return items
    try:
        soup = BeautifulSoup(html, 'lxml')
        cards = soup.select('.Product')[:ITEMS_PER_PAGE]
        for card in cards:
            item_data = extract_item_from_card(
                card,
                source='Yahoo Flea',
                base_url='https://paypayfleamarket.yahoo.co.jp',
                title_sel='.Product__titleLink',
                price_sel='.Product__price',
                link_sel='a',
                img_sel='img'
            )
            if item_data:
                item_data['id'] = generate_item_id(item_data)
                items.append(item_data)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Yahoo Flea –¥–ª—è {keyword}: {e}")
    return items

async def parse_yahoo_auction_async(session, keyword, semaphore):
    items = []
    url = f"https://auctions.yahoo.co.jp/search/search?p={quote(keyword)}&aq=-1&type=all&auccat=&tab_ex=commerce&order=desc"
    html = await fetch_with_fallback(session, url, semaphore, expected_selector='.Product')
    if not html:
        return items
    try:
        soup = BeautifulSoup(html, 'lxml')
        cards = soup.select('.Product')[:ITEMS_PER_PAGE]
        for card in cards:
            item_data = extract_item_from_card(
                card,
                source='Yahoo Auction',
                base_url='https://auctions.yahoo.co.jp',
                title_sel='.Product__titleLink',
                price_sel='.Product__price',
                link_sel='a',
                img_sel='img'
            )
            if item_data:
                item_data['id'] = generate_item_id(item_data)
                items.append(item_data)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Yahoo Auction –¥–ª—è {keyword}: {e}")
    return items

async def parse_yahoo_shopping_async(session, keyword, semaphore):
    items = []
    url = f"https://shopping.yahoo.co.jp/search?p={quote(keyword)}&used=1&order=desc&sort=create_time"
    html = await fetch_with_fallback(session, url, semaphore, expected_selector='.Loop__item')
    if not html:
        return items
    try:
        soup = BeautifulSoup(html, 'lxml')
        cards = soup.select('.Loop__item')[:ITEMS_PER_PAGE]
        for card in cards:
            item_data = extract_item_from_card(
                card,
                source='Yahoo Shopping',
                base_url='https://shopping.yahoo.co.jp',
                title_sel='.Loop__itemTitle a',
                price_sel='.Loop__itemPrice',
                link_sel='a',
                img_sel='img'
            )
            if item_data:
                item_data['id'] = generate_item_id(item_data)
                items.append(item_data)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Yahoo Shopping –¥–ª—è {keyword}: {e}")
    return items

async def parse_rakuten_mall_async(session, keyword, semaphore):
    items = []
    url = f"https://search.rakuten.co.jp/search/mall/{quote(keyword)}/?used=1"
    html = await fetch_with_fallback(session, url, semaphore, expected_selector='.searchresultitem')
    if not html:
        alt_url = f"https://search.rakuten.co.jp/search/mall/?v=2&p={quote(keyword)}&used=1"
        html = await fetch_with_fallback(session, alt_url, semaphore, expected_selector='.searchresultitem')
        if not html:
            return items
    try:
        soup = BeautifulSoup(html, 'lxml')
        cards = soup.select('.searchresultitem')[:ITEMS_PER_PAGE]
        for card in cards:
            item_data = extract_item_from_card(
                card,
                source='Rakuten Mall',
                base_url='https://search.rakuten.co.jp',
                title_sel='.title a',
                price_sel='.important',
                link_sel='a',
                img_sel='img'
            )
            if item_data:
                item_data['id'] = generate_item_id(item_data)
                items.append(item_data)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Rakuten Mall –¥–ª—è {keyword}: {e}")
    return items

async def parse_ebay_async(session, keyword, semaphore):
    items = []
    url = f"https://www.ebay.com/sch/i.html?_nkw={quote(keyword)}&_sacat=11450&LH_ItemCondition=4&_sop=10"
    html = await fetch_with_fallback(session, url, semaphore, expected_selector='li.s-item')
    if not html:
        return items
    try:
        soup = BeautifulSoup(html, 'lxml')
        cards = soup.select('li.s-item')[:ITEMS_PER_PAGE]
        for card in cards:
            title_elem = card.select_one('.s-item__title')
            if not title_elem or 'Shop on' in title_elem.text:
                continue
            item_data = extract_item_from_card(
                card,
                source='eBay',
                base_url='https://www.ebay.com',
                title_sel='.s-item__title',
                price_sel='.s-item__price',
                link_sel='a.s-item__link',
                img_sel='.s-item__image-img'
            )
            if item_data:
                item_data['id'] = generate_item_id(item_data)
                items.append(item_data)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ eBay –¥–ª—è {keyword}: {e}")
    return items

async def parse_2ndstreet_async(session, keyword, semaphore):
    items = []
    url = f"https://www.2ndstreet.jp/search?keyword={quote(keyword)}"
    html = await fetch_with_fallback(session, url, semaphore, expected_selector='.itemList .item')
    if not html:
        return items
    try:
        soup = BeautifulSoup(html, 'lxml')
        cards = soup.select('.itemList .item')[:ITEMS_PER_PAGE]
        for card in cards:
            item_data = extract_item_from_card(
                card,
                source='2nd Street JP',
                base_url='https://www.2ndstreet.jp',
                title_sel='.itemName',
                price_sel='.price',
                link_sel='a',
                img_sel='img'
            )
            if item_data:
                item_data['id'] = generate_item_id(item_data)
                items.append(item_data)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ 2nd Street –¥–ª—è {keyword}: {e}")
    return items

# ==================== –°–ª–æ–≤–∞—Ä—å –ø–∞—Ä—Å–µ—Ä–æ–≤ ====================
ASYNC_PARSERS = {
    'Mercari JP': parse_mercari_async,
    'Rakuten Rakuma': parse_rakuma_async,
    'Yahoo Flea': parse_yahoo_flea_async,
    'Yahoo Auction': parse_yahoo_auction_async,
    'Yahoo Shopping': parse_yahoo_shopping_async,
    'Rakuten Mall': parse_rakuten_mall_async,
    'eBay': parse_ebay_async,
    '2nd Street JP': parse_2ndstreet_async,
}

# ==================== –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ ====================
async def search_all_async(keywords, platforms, max_concurrent=20):
    semaphore = asyncio.Semaphore(max_concurrent)
    connector = aiohttp.TCPConnector(limit=100, limit_per_host=10, ttl_dns_cache=300, ssl=False)
    timeout = aiohttp.ClientTimeout(total=30)
    
    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
        tasks = []
        for platform in platforms:
            if platform in ASYNC_PARSERS:
                parser = ASYNC_PARSERS[platform]
                for keyword in keywords:
                    tasks.append(parser(session, keyword, semaphore))
        
        logger.info(f"üöÄ –ó–∞–ø—É—â–µ–Ω–æ {len(tasks)} –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á")
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        all_items = []
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"–û—à–∏–±–∫–∞ –≤ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –∑–∞–¥–∞—á–µ: {result}")
            elif isinstance(result, list):
                all_items.extend(result)
        
        logger.info(f"‚úÖ –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω, –Ω–∞–π–¥–µ–Ω–æ {len(all_items)} —Ç–æ–≤–∞—Ä–æ–≤")
        return all_items

# ==================== –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∏–∑ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∞ ====================
def run_async_search(keywords, platforms, max_concurrent=20):
    from async_loop import run_coro
    future = run_coro(search_all_async(keywords, platforms, max_concurrent))
    return future.result()