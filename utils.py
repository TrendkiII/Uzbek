import hashlib
import time
import random
import requests
import logging
import json
from urllib.parse import urljoin, quote
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
from config import (
    PROXY_POOL, USE_PROXY_POOL, proxy_lock, PROXY_FILE,
    REQUEST_TIMEOUT, MAX_RETRIES, RETRY_DELAY,
    USER_AGENTS_COUNT, BROWSER_HEADERS,
    MIN_DELAY_BETWEEN_REQUESTS, MAX_DELAY_BETWEEN_REQUESTS,
    MIN_DELAY_BETWEEN_BRANDS, MAX_DELAY_BETWEEN_BRANDS,
    REQUESTS_BEFORE_PROXY_CHANGE
)
from concurrent.futures import ThreadPoolExecutor, as_completed

logger = logging.getLogger(__name__)

# ================== User-Agent ==================
ua = UserAgent()
USER_AGENTS = [ua.random for _ in range(USER_AGENTS_COUNT)]
UA_INDEX = 0
ua_lock = Lock()

def get_next_user_agent():
    with ua_lock:
        global UA_INDEX
        agent = USER_AGENTS[UA_INDEX % len(USER_AGENTS)]
        UA_INDEX += 1
        return agent

# ================== –ü—Ä–æ–∫—Å–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–Ω—ã—Ö –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤ ==================
request_counter = 0
current_proxy_index = 0
bad_proxies = set()

def load_proxies_from_file():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å–∏ –∏–∑ —Ñ–∞–π–ª–∞"""
    try:
        with open(PROXY_FILE, 'r') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return []

def save_proxies_to_file(proxies):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å–∏ –≤ —Ñ–∞–π–ª"""
    with open(PROXY_FILE, 'w') as f:
        json.dump(proxies, f, indent=2)

def init_proxy_pool():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç PROXY_POOL –∏–∑ —Ñ–∞–π–ª–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ"""
    global PROXY_POOL
    with proxy_lock:
        PROXY_POOL = load_proxies_from_file()
        logger.info(f"üì¶ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(PROXY_POOL)} –ø—Ä–æ–∫—Å–∏ –∏–∑ —Ñ–∞–π–ª–∞")

def add_proxy_to_pool(proxy_url):
    """–î–æ–±–∞–≤–ª—è–µ—Ç –ø—Ä–æ–∫—Å–∏ –≤ –ø—É–ª –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ —Ñ–∞–π–ª"""
    with proxy_lock:
        if proxy_url not in PROXY_POOL:
            PROXY_POOL.append(proxy_url)
            save_proxies_to_file(PROXY_POOL)
            logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω –ø—Ä–æ–∫—Å–∏: {proxy_url}")
            return True
    return False

def remove_proxy_from_pool(proxy_url):
    """–£–¥–∞–ª—è–µ—Ç –ø—Ä–æ–∫—Å–∏ –∏–∑ –ø—É–ª–∞ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç"""
    with proxy_lock:
        if proxy_url in PROXY_POOL:
            PROXY_POOL.remove(proxy_url)
            save_proxies_to_file(PROXY_POOL)
            logger.info(f"üóë –£–¥–∞–ª—ë–Ω –ø—Ä–æ–∫—Å–∏: {proxy_url}")
            return True
    return False

def test_proxy(proxy_url):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–∫—Å–∏"""
    proxies = {'http': proxy_url, 'https': proxy_url}
    try:
        start = time.time()
        r = requests.get('http://httpbin.org/ip', proxies=proxies, timeout=10)
        if r.status_code == 200:
            elapsed = time.time() - start
            return proxy_url, True, r.json().get('origin'), round(elapsed, 2)
    except:
        pass
    return proxy_url, False, None, None

def check_and_update_proxies(proxy_list=None):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å–∏ (–µ—Å–ª–∏ None ‚Äì –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Ç–µ–∫—É—â–∏–π PROXY_POOL),
    –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞–±–æ—á–∏–µ –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç —Ñ–∞–π–ª, —É–¥–∞–ª—è—è –Ω–µ—Ä–∞–±–æ—á–∏–µ.
    """
    if proxy_list is None:
        with proxy_lock:
            proxy_list = PROXY_POOL.copy()
    
    if not proxy_list:
        return []
    
    working = []
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = {executor.submit(test_proxy, p): p for p in proxy_list}
        for future in as_completed(futures):
            proxy, ok, ip, speed = future.result()
            if ok:
                working.append(proxy)
                logger.info(f"‚úÖ {proxy} —Ä–∞–±–æ—Ç–∞–µ—Ç (IP: {ip}, {speed}—Å)")
            else:
                logger.warning(f"‚ùå {proxy} –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç")
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –ø—É–ª –∏ —Ñ–∞–π–ª, –µ—Å–ª–∏ –ø—Ä–æ–≤–µ—Ä—è–ª–∏ —Ç–µ–∫—É—â–∏–π –ø—É–ª
    if proxy_list is PROXY_POOL or proxy_list == PROXY_POOL:
        with proxy_lock:
            PROXY_POOL[:] = working
            save_proxies_to_file(PROXY_POOL)
    
    return working

def get_next_proxy():
    """–†–æ—Ç–∞—Ü–∏—è –ø—Ä–æ–∫—Å–∏ —Å —É—á—ë—Ç–æ–º —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö"""
    if not USE_PROXY_POOL or not PROXY_POOL:
        return None
    
    with proxy_lock:
        global request_counter, current_proxy_index
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–∏–µ –ø—Ä–æ–∫—Å–∏
        available_proxies = [p for p in PROXY_POOL if p not in bad_proxies]
        
        if not available_proxies:
            logger.warning("‚ö†Ô∏è –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏!")
            return None
        
        # –°–º–µ–Ω–∞ –ø—Ä–æ–∫—Å–∏ –∫–∞–∂–¥—ã–µ N –∑–∞–ø—Ä–æ—Å–æ–≤
        if request_counter >= REQUESTS_BEFORE_PROXY_CHANGE:
            current_proxy_index = (current_proxy_index + 1) % len(available_proxies)
            request_counter = 0
            logger.info(f"üîÑ –°–º–µ–Ω–∞ –ø—Ä–æ–∫—Å–∏ –Ω–∞ {available_proxies[current_proxy_index]}")
        
        proxy_url = available_proxies[current_proxy_index]
        request_counter += 1
        
        return {
            'http': proxy_url,
            'https': proxy_url
        }

def mark_proxy_bad(proxy_dict):
    """–ü–æ–º–µ—á–∞–µ—Ç –ø—Ä–æ–∫—Å–∏ –∫–∞–∫ –Ω–µ—Ä–∞–±–æ—Ç–∞—é—â–∏–π"""
    if not proxy_dict:
        return
    
    with proxy_lock:
        for p in PROXY_POOL:
            proxy_url = proxy_dict.get('http', '')
            if proxy_url and proxy_url in p:
                bad_proxies.add(p)
                logger.warning(f"üóë –ü—Ä–æ–∫—Å–∏ {p} –ø–æ–º–µ—á–µ–Ω –∫–∞–∫ –Ω–µ—Ä–∞–±–æ—Ç–∞—é—â–∏–π")
                break

def get_proxy_stats():
    with proxy_lock:
        return {
            'total': len(PROXY_POOL),
            'bad': len(bad_proxies),
            'good': len(PROXY_POOL) - len(bad_proxies),
            'current_index': current_proxy_index,
            'requests_this_proxy': request_counter
        }

# ================== –ì–µ–Ω–µ—Ä–∞—Ü–∏—è ID ==================
def generate_item_id(item):
    unique = f"{item['source']}_{item['url']}_{item['title']}"
    return hashlib.md5(unique.encode('utf-8')).hexdigest()

# ================== CSS —Å–µ–ª–µ–∫—Ç–æ—Ä ==================
def safe_select(element, selectors):
    for sel in selectors:
        elem = element.select_one(sel)
        if elem:
            return elem
    return None

# ================== –ó–∞–¥–µ—Ä–∂–∫–∏ ==================
def human_delay(min_sec=MIN_DELAY_BETWEEN_REQUESTS, max_sec=MAX_DELAY_BETWEEN_REQUESTS):
    time.sleep(random.uniform(min_sec, max_sec))

def brand_delay():
    time.sleep(random.uniform(MIN_DELAY_BETWEEN_BRANDS, MAX_DELAY_BETWEEN_BRANDS))

# ================== HTTP –∑–∞–ø—Ä–æ—Å—ã —Å –º–∞—Å–∫–∏—Ä–æ–≤–∫–æ–π ==================
def make_request(url, headers=None, timeout=REQUEST_TIMEOUT, retries=MAX_RETRIES):
    if headers is None:
        headers = BROWSER_HEADERS.copy()
    
    headers['User-Agent'] = get_next_user_agent()
    
    if random.random() > 0.7:
        headers['Referer'] = random.choice([
            'https://www.google.com/',
            'https://www.yahoo.com/',
            'https://www.bing.com/',
        ])

    for attempt in range(retries):
        proxies = get_next_proxy()
        
        if attempt == 0:
            human_delay()
        else:
            time.sleep(RETRY_DELAY * (2 ** attempt))
        
        try:
            logger.debug(f"üåê –ó–∞–ø—Ä–æ—Å {url[:100]}... —á–µ—Ä–µ–∑ {proxies.get('http') if proxies else '–±–µ–∑ –ø—Ä–æ–∫—Å–∏'}")
            
            r = requests.get(
                url, 
                headers=headers, 
                timeout=timeout, 
                proxies=proxies,
                allow_redirects=True
            )
            r.raise_for_status()
            return r
            
        except requests.exceptions.ProxyError as e:
            logger.warning(f"üîå –û—à–∏–±–∫–∞ –ø—Ä–æ–∫—Å–∏ {attempt+1}/{retries}: {e}")
            if proxies:
                mark_proxy_bad(proxies)
            
        except requests.exceptions.Timeout:
            logger.warning(f"‚è∞ –¢–∞–π–º–∞—É—Ç {attempt+1}/{retries} –¥–ª—è {url[:50]}...")
            
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 403:
                logger.warning(f"üö´ 403 Forbidden - –º–µ–Ω—è–µ–º User-Agent")
                headers['User-Agent'] = get_next_user_agent()
            elif e.response.status_code == 429:
                logger.warning(f"üõë 429 Too Many Requests - –∂–¥—ë–º")
                time.sleep(60)
            elif e.response.status_code == 404:
                return None
            else:
                logger.warning(f"üåê HTTP –æ—à–∏–±–∫–∞ {e.response.status_code}")
                
        except requests.exceptions.ConnectionError:
            logger.warning(f"üîå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è {attempt+1}/{retries}")
            
        except Exception as e:
            logger.warning(f"‚ùå –û—à–∏–±–∫–∞ {attempt+1}/{retries}: {e}")
    
    logger.error(f"‚ùå –í—Å–µ {retries} –ø–æ–ø—ã—Ç–æ–∫ –Ω–µ —É–¥–∞–ª–∏—Å—å –¥–ª—è {url[:50]}...")
    return None

# ================== URL helpers ==================
def make_full_url(base, href):
    if not href:
        return ''
    if href.startswith('http'):
        return href
    return urljoin(base, href)

def encode_keyword(keyword):
    return quote(keyword)