import hashlib
import time
import random
import requests
import logging
from urllib.parse import urljoin, quote
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
from config import PROXY, REQUEST_TIMEOUT, MAX_RETRIES, RETRY_DELAY

logger = logging.getLogger(__name__)

# ================== –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ User-Agent ==================
ua = UserAgent()
USER_AGENTS = [ua.random for _ in range(20)]
UA_INDEX = 0

def get_next_user_agent():
    global UA_INDEX
    agent = USER_AGENTS[UA_INDEX % len(USER_AGENTS)]
    UA_INDEX += 1
    return agent

# ================== –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ ID –¥–ª—è —Ç–æ–≤–∞—Ä–∞ ==================
def generate_item_id(item):
    """
    –§–æ—Ä–º–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –Ω–∞ –æ—Å–Ω–æ–≤–µ source, url –∏ title.
    """
    unique = f"{item['source']}_{item['url']}_{item['title']}"
    return hashlib.md5(unique.encode('utf-8')).hexdigest()

# ================== –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π CSS —Å–µ–ª–µ–∫—Ç–æ—Ä ==================
def safe_select(element, selectors):
    """
    –ü—Ä–æ–±—É–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–≤—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç.
    –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç None.
    """
    for sel in selectors:
        elem = element.select_one(sel)
        if elem:
            return elem
    return None

# ================== –ë–µ–∑–æ–ø–∞—Å–Ω—ã–µ HTTP-–∑–∞–ø—Ä–æ—Å—ã —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º ==================
def make_request(url, headers=None, timeout=REQUEST_TIMEOUT, retries=MAX_RETRIES):
    """
    –î–µ–ª–∞–µ—Ç GET-–∑–∞–ø—Ä–æ—Å —Å –ø–æ–≤—Ç–æ—Ä–∞–º–∏, —Ä–æ—Ç–∞—Ü–∏–µ–π User-Agent –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–æ–∫—Å–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–∫—Ç Response –∏–ª–∏ None.
    """
    if headers is None:
        headers = {'User-Agent': get_next_user_agent()}

    proxies = {'http': PROXY, 'https': PROXY} if PROXY else None

    for attempt in range(retries):
        try:
            r = requests.get(url, headers=headers, timeout=timeout, proxies=proxies)
            r.raise_for_status()
            return r
        except requests.exceptions.Timeout:
            logger.warning(f"‚è∞ –¢–∞–π–º–∞—É—Ç {attempt+1}/{retries} –¥–ª—è {url}")
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 403:
                logger.warning(f"üö´ 403 Forbidden: {url} ‚Äì –º–µ–Ω—è–µ–º User-Agent")
                headers = {'User-Agent': get_next_user_agent()}
            elif e.response.status_code == 404:
                logger.warning(f"üîç 404 Not Found: {url} ‚Äì —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            else:
                logger.warning(f"üåê HTTP –æ—à–∏–±–∫–∞ {attempt+1}/{retries} –¥–ª—è {url}: {e}")
        except requests.exceptions.ConnectionError:
            logger.warning(f"üîå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è {attempt+1}/{retries} –¥–ª—è {url}")
        except Exception as e:
            logger.warning(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ {attempt+1}/{retries} –¥–ª—è {url}: {e}")

        if attempt < retries - 1:
            time.sleep(RETRY_DELAY * (attempt + 1))
    return None

# ================== URL –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ ==================
def make_full_url(base, href):
    """
    –ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π href –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π URL.
    """
    if not href:
        return ''
    if href.startswith('http'):
        return href
    return urljoin(base, href)

def encode_keyword(keyword):
    """
    –ö–æ–¥–∏—Ä—É–µ—Ç –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –¥–ª—è URL (–Ω–∞–ø—Ä–∏–º–µ—Ä –¥–ª—è —è–ø–æ–Ω—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤)
    """
    return quote(keyword)